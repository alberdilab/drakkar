import os
import gzip
import json
import pandas as pd
from glob import glob

####
# Define config variables
####

configfile: "workflow/config.yaml"
WORKFLOW = config.get("workflow", None)
OUTPUT_DIR = config.get("output_dir", None)
READS_DIR = config.get("reads_dir", None)
PREPROCESS_DIR = config.get("preprocess_dir", None)
REFERENCE = config.get("reference", None)
CATALOGING_MODE = config["cataloging_mode"].split(",") if "cataloging_mode" in config else []

####
# Calculate optimal resources for computing
####
def calculate_file_sizes(folder):
    file_sizes = {}
    for file in os.listdir(folder):
        if file.endswith("1.fq.gz") or file.endswith(".fna"):
            filepath = os.path.join(folder, file)
            # Get file size in bytes and convert to megabytes
            size_mb = os.path.getsize(filepath) / (1024 ** 2)
            file_sizes[file] = size_mb
    return file_sizes

def calculate_file_size(filepath):
    file_size = os.path.getsize(filepath) / (1024 ** 2)
    return file_size

####
# Run pipelines
####

# Run preparing
include: "rules/preparing.smk"

if WORKFLOW == "complete":
    rule all:
        input:
            "results/complete.txt"

    include: "rules/preprocessing.smk"
    include: "rules/cataloging.smk"
    include: "rules/annotation.smk"
    include: "rules/quantification.smk"

if WORKFLOW == "preprocessing":

    # Check if reference is provided
    USE_REFERENCE = REFERENCE not in [None, "None"]

    with open("sample_to_reads1.json", "r") as f:
        SAMPLE_TO_READS1 = json.load(f)

    with open("sample_to_reads2.json", "r") as f:
        SAMPLE_TO_READS2 = json.load(f)
    samples = list(SAMPLE_TO_READS1.keys())

    rule all:
        input:
            expand(f"{OUTPUT_DIR}/preprocessing/final/{{sample}}_1.fq.gz", sample=samples),
            expand(f"{OUTPUT_DIR}/preprocessing/final/{{sample}}_2.fq.gz", sample=samples),
            *([expand(f"{OUTPUT_DIR}/preprocessing/final/{{sample}}.bam", sample=samples)] if USE_REFERENCE else [])

    include: "rules/preprocessing.smk"

if WORKFLOW == "cataloging":

    samples, = glob_wildcards(f"{PREPROCESS_DIR}/{{sample}}_1.fq.gz")

    preprocess_mb = calculate_file_sizes(PREPROCESS_DIR)
    preprocess_mb = {key.replace('_1.fq.gz', ''): value for key, value in preprocess_mb.items()}
    preprocess_mb_total = sum(preprocess_mb.values())

    # A single cataloging rule is created, as multiple cataloging modes can be combined
    rule all:
        input:
            *([expand(f"{OUTPUT_DIR}/cataloging/metabat2/{{sample}}/{{sample}}.tsv", sample=samples)] if "individual" in CATALOGING_MODE else []),
            *([expand(f"{OUTPUT_DIR}/cataloging/maxbin2/{{sample}}/{{sample}}.tsv", sample=samples)] if "individual" in CATALOGING_MODE else []),
            *([f"{OUTPUT_DIR}/cataloging/metabat2/all/all.tsv"] if "all" in CATALOGING_MODE else []),
            *([f"{OUTPUT_DIR}/cataloging/maxbin2/all/all.tsv"] if "all" in CATALOGING_MODE else [])

    if "individual" in CATALOGING_MODE:
        include: "rules/cataloging_ind.smk"

    if "all" in CATALOGING_MODE:
        include: "rules/cataloging_all.smk"
