import os
import gzip
import json
import pandas as pd
from glob import glob

####
# Define config variables
####

configfile: "workflow/config.yaml"
WORKFLOW = config.get("workflow", None)
OUTPUT_DIR = config.get("output_dir", None)
REFERENCE = config.get("reference", None)
CATALOGING_MODE = config["cataloging_mode"].split(",") if "cataloging_mode" in config else []

####
# Calculate optimal resources for computing
####

import os

def calculate_file_sizes(file_dict):
    """Calculates file sizes in megabytes (MB) for both single-file and list-based dictionaries."""
    file_sizes = {}

    for key, value in file_dict.items():
        total_size_mb = 0  # Initialize total size for this key

        # If value is a single string (single file path)
        if isinstance(value, str):
            files = [value]  # Convert to list to handle uniformly
        else:
            files = value  # Already a list

        # Process each file
        for file_path in files:
            if os.path.exists(file_path):  # Check if file exists
                file_size_mb = os.path.getsize(file_path) / 1e6  # Convert bytes to MB
                total_size_mb += file_size_mb
            else:
                print(f"⚠️ Warning: File not found - {file_path}")

        file_sizes[key] = total_size_mb  # Store total size for this key

    return file_sizes


####
# Run pipelines
####

if WORKFLOW == "complete":

    with open(f"{OUTPUT_DIR}/data/sample_to_reads1.json", "r") as f:
        SAMPLE_TO_READS1 = json.load(f)

    with open(f"{OUTPUT_DIR}/data/sample_to_reads2.json", "r") as f:
        SAMPLE_TO_READS2 = json.load(f)

    with open(f"{OUTPUT_DIR}/data/reference_to_file.json", "r") as f:
        REFERENCE_TO_FILE = json.load(f)

    with open(f"{OUTPUT_DIR}/data/sample_to_reference.json", "r") as f:
        SAMPLE_TO_REFERENCE = json.load(f)

    with open(f"{OUTPUT_DIR}/data/preprocessed_to_reads1.json", "r") as f:
        PREPROCESSED_TO_READS1 = json.load(f)

    with open(f"{OUTPUT_DIR}/data/preprocessed_to_reads2.json", "r") as f:
        PREPROCESSED_TO_READS2 = json.load(f)

    with open(f"{OUTPUT_DIR}/data/assembly_to_samples.json", "r") as f:
        ASSEMBLY_TO_SAMPLES = json.load(f)

    samples = list(SAMPLE_TO_READS1.keys())
    references = list(REFERENCE_TO_FILE.keys())
    assemblies = list(ASSEMBLY_TO_SAMPLES.keys())

    reads_mb = calculate_file_sizes(SAMPLE_TO_READS1)
    reads_mb_total = sum(reads_mb.values())

    reference_mb = calculate_file_sizes(REFERENCE_TO_FILE)
    reference_mb_total = sum(reference_mb.values())

    preprocessed_mb = calculate_file_sizes(PREPROCESSED_TO_READS1)
    preprocessed_mb_total = sum(preprocessed_mb.values())

    rule all:
        input:
            expand(f"{OUTPUT_DIR}/cataloging/final/{{assembly}}.tsv", assembly=assemblies)

    if REFERENCE:
        include: "rules/preparing.smk"
        include: "rules/preprocessing_ref.smk"
        include: "rules/cataloging.smk"
    if not REFERENCE:
        include: "rules/preparing.smk"
        include: "rules/preprocessing.smk"
        include: "rules/cataloging.smk"

if WORKFLOW == "preprocessing":

    with open(f"{OUTPUT_DIR}/data/sample_to_reads1.json", "r") as f:
        SAMPLE_TO_READS1 = json.load(f)

    with open(f"{OUTPUT_DIR}/data/sample_to_reads2.json", "r") as f:
        SAMPLE_TO_READS2 = json.load(f)

    with open(f"{OUTPUT_DIR}/data/reference_to_file.json", "r") as f:
        REFERENCE_TO_FILE = json.load(f)

    with open(f"{OUTPUT_DIR}/data/sample_to_reference.json", "r") as f:
        SAMPLE_TO_REFERENCE = json.load(f)

    samples = list(SAMPLE_TO_READS1.keys())
    references = list(REFERENCE_TO_FILE.keys())

    reads_mb = calculate_file_sizes(SAMPLE_TO_READS1)
    reads_mb_total = sum(reads_mb.values())

    reference_mb = calculate_file_sizes(REFERENCE_TO_FILE)
    reference_mb_total = sum(reference_mb.values())

    # The rules reference_index, reference_map, metagenomic_reads and host_reads are only run if
    # the reference genome file is provided. Otherwise, the fastp rule already outputs the final files.

    if REFERENCE:
        rule all:
            input:
                expand(f"{OUTPUT_DIR}/data/references/{{reference}}.fna", reference=references),
                expand(f"{OUTPUT_DIR}/data/references/{{reference}}.rev.1.bt2", reference=references),
                expand(f"{OUTPUT_DIR}/preprocessing/final/{{sample}}_1.fq.gz", sample=samples),
                expand(f"{OUTPUT_DIR}/preprocessing/final/{{sample}}_2.fq.gz", sample=samples),
                expand(f"{OUTPUT_DIR}/preprocessing/final/{{sample}}.bam", sample=samples)

        include: "rules/preparing.smk"
        include: "rules/preprocessing_ref.smk"

    if not REFERENCE:
        rule all:
            input:
                expand(f"{OUTPUT_DIR}/preprocessing/final/{{sample}}_1.fq.gz", sample=samples),
                expand(f"{OUTPUT_DIR}/preprocessing/final/{{sample}}_2.fq.gz", sample=samples)

        include: "rules/preparing.smk"
        include: "rules/preprocessing.smk"

if WORKFLOW == "cataloging":

    with open(f"{OUTPUT_DIR}/data/preprocessed_to_reads1.json", "r") as f:
        PREPROCESSED_TO_READS1 = json.load(f)

    with open(f"{OUTPUT_DIR}/data/preprocessed_to_reads2.json", "r") as f:
        PREPROCESSED_TO_READS2 = json.load(f)

    with open(f"{OUTPUT_DIR}/data/assembly_to_samples.json", "r") as f:
        ASSEMBLY_TO_SAMPLES = json.load(f)

    preprocessed_mb = calculate_file_sizes(PREPROCESSED_TO_READS1)
    preprocessed_mb_total = sum(preprocessed_mb.values())

    samples = list(PREPROCESSED_TO_READS1.keys())
    assemblies = list(ASSEMBLY_TO_SAMPLES.keys())

    rule all:
        input:
            expand(f"{OUTPUT_DIR}/cataloging/final/{{assembly}}.tsv", assembly=assemblies)

    include: "rules/cataloging.smk"
