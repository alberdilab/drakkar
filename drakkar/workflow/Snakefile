import os
import gzip
import json
import pandas as pd
from glob import glob

####
# Define config variables
####

configfile: "workflow/config.yaml"
WORKFLOW = config.get("workflow", None)
OUTPUT_DIR = config.get("output_dir", None)
REFERENCE = config.get("reference", None)
PROFILING_TYPE = config["profiling_type"].split(",") if "profiling_type" in config else []

####
# Calculate optimal resources for computing
####

import os

def calculate_file_sizes(file_dict):
    """Calculates file sizes in megabytes (MB) for both single-file and list-based dictionaries."""
    file_sizes = {}

    for key, value in file_dict.items():
        total_size_mb = 0  # Initialize total size for this key

        # If value is a single string (single file path)
        if isinstance(value, str):
            files = [value]  # Convert to list to handle uniformly
        else:
            files = value  # Already a list

        # Process each file
        for file_path in files:
            if os.path.exists(file_path):  # Check if file exists
                file_size_mb = os.path.getsize(file_path) / 1e6  # Convert bytes to MB
                total_size_mb += file_size_mb
            else:
                print(f"⚠️ Warning: File not found - {file_path}")

        file_sizes[key] = total_size_mb  # Store total size for this key

    return file_sizes


####
# Run pipelines
####

if WORKFLOW == "complete":

    with open(f"{OUTPUT_DIR}/data/sample_to_reads1.json", "r") as f:
        SAMPLE_TO_READS1 = json.load(f)

    with open(f"{OUTPUT_DIR}/data/sample_to_reads2.json", "r") as f:
        SAMPLE_TO_READS2 = json.load(f)

    with open(f"{OUTPUT_DIR}/data/reference_to_file.json", "r") as f:
        REFERENCE_TO_FILE = json.load(f)

    with open(f"{OUTPUT_DIR}/data/sample_to_reference.json", "r") as f:
        SAMPLE_TO_REFERENCE = json.load(f)

    with open(f"{OUTPUT_DIR}/data/preprocessed_to_reads1.json", "r") as f:
        PREPROCESSED_TO_READS1 = json.load(f)

    with open(f"{OUTPUT_DIR}/data/preprocessed_to_reads2.json", "r") as f:
        PREPROCESSED_TO_READS2 = json.load(f)

    with open(f"{OUTPUT_DIR}/data/assembly_to_samples.json", "r") as f:
        ASSEMBLY_TO_SAMPLES = json.load(f)

    samples = list(SAMPLE_TO_READS1.keys())
    references = list(REFERENCE_TO_FILE.keys())
    assemblies = list(ASSEMBLY_TO_SAMPLES.keys())

    reads_mb = calculate_file_sizes(SAMPLE_TO_READS1)
    reads_mb_total = sum(reads_mb.values())

    reference_mb = calculate_file_sizes(REFERENCE_TO_FILE)
    reference_mb_total = sum(reference_mb.values())

    preprocessed_mb = calculate_file_sizes(PREPROCESSED_TO_READS1)
    preprocessed_mb_total = sum(preprocessed_mb.values())

    rule all:
        input:
            expand(f"{OUTPUT_DIR}/cataloging/final/{{assembly}}.tsv", assembly=assemblies)

    if REFERENCE:
        include: "rules/preparing.smk"
        include: "rules/preprocessing_ref.smk"
        include: "rules/cataloging.smk"
    if not REFERENCE:
        include: "rules/preparing.smk"
        include: "rules/preprocessing.smk"
        include: "rules/cataloging.smk"

if WORKFLOW == "preprocessing":

    with open(f"{OUTPUT_DIR}/data/sample_to_reads1.json", "r") as f:
        SAMPLE_TO_READS1 = json.load(f)

    with open(f"{OUTPUT_DIR}/data/sample_to_reads2.json", "r") as f:
        SAMPLE_TO_READS2 = json.load(f)

    samples = list(SAMPLE_TO_READS1.keys())

    # The rules reference_index, reference_map, metagenomic_reads and host_reads are only run if
    # the reference genome file is provided. Otherwise, the fastp rule already outputs the final files.

    if REFERENCE:

        with open(f"{OUTPUT_DIR}/data/reference_to_file.json", "r") as f:
            REFERENCE_TO_FILE = json.load(f)

        with open(f"{OUTPUT_DIR}/data/sample_to_reference.json", "r") as f:
            SAMPLE_TO_REFERENCE = json.load(f)

        references = list(REFERENCE_TO_FILE.keys())

        rule all:
            input:
                expand(f"{OUTPUT_DIR}/data/references/{{reference}}.fna", reference=references),
                expand(f"{OUTPUT_DIR}/data/references/{{reference}}.rev.1.bt2", reference=references),
                expand(f"{OUTPUT_DIR}/preprocessing/final/{{sample}}_1.fq.gz", sample=samples),
                expand(f"{OUTPUT_DIR}/preprocessing/final/{{sample}}_2.fq.gz", sample=samples),
                expand(f"{OUTPUT_DIR}/preprocessing/final/{{sample}}.bam", sample=samples),
                f"{OUTPUT_DIR}/preprocessing.tsv",
                f"{OUTPUT_DIR}/drakkar_report.html",
                f"{OUTPUT_DIR}/data/preprocessing_report.done"

        include: "rules/preparing.smk"
        include: "rules/preprocessing_ref.smk"

    if not REFERENCE:
        rule all:
            input:
                expand(f"{OUTPUT_DIR}/preprocessing/final/{{sample}}_1.fq.gz", sample=samples),
                expand(f"{OUTPUT_DIR}/preprocessing/final/{{sample}}_2.fq.gz", sample=samples),
                f"{OUTPUT_DIR}/preprocessing.tsv",
                f"{OUTPUT_DIR}/drakkar_report.html",
                f"{OUTPUT_DIR}/data/preprocessing_report.done"

        include: "rules/preparing.smk"
        include: "rules/preprocessing.smk"

if WORKFLOW == "cataloging":

    with open(f"{OUTPUT_DIR}/data/preprocessed_to_reads1.json", "r") as f:
        PREPROCESSED_TO_READS1 = json.load(f)

    with open(f"{OUTPUT_DIR}/data/preprocessed_to_reads2.json", "r") as f:
        PREPROCESSED_TO_READS2 = json.load(f)

    with open(f"{OUTPUT_DIR}/data/assembly_to_samples.json", "r") as f:
        ASSEMBLY_TO_SAMPLES = json.load(f)

    preprocessed_mb = calculate_file_sizes(PREPROCESSED_TO_READS1)
    preprocessed_mb_total = sum(preprocessed_mb.values())

    samples = list(PREPROCESSED_TO_READS1.keys())
    assemblies = list(ASSEMBLY_TO_SAMPLES.keys())

    #Function to obtain bin_ids dynamically when they are generated by binette
    def get_bin_ids_from_tsv(tsv_path):
        df = pd.read_csv(tsv_path, sep="\t")
        return df["bin_id"].unique()

    rule all:
        input:
            f"{OUTPUT_DIR}/cataloging/final/all_bin_paths.txt",
            f"{OUTPUT_DIR}/cataloging/final/all_bin_metadata.csv",
            [f"{OUTPUT_DIR}/cataloging/final/{assembly}/{assembly}_bin_{bin_id}.fa"
            for assembly in assemblies
            for bin_id in get_bin_ids_from_tsv(f"{OUTPUT_DIR}/cataloging/binette/{assembly}/final_bins_quality_reports.tsv")]

    include: "rules/cataloging.smk"

if WORKFLOW == "profiling":

    with open(f"{OUTPUT_DIR}/data/preprocessed_to_reads1.json", "r") as f:
        PREPROCESSED_TO_READS1 = json.load(f)

    with open(f"{OUTPUT_DIR}/data/preprocessed_to_reads2.json", "r") as f:
        PREPROCESSED_TO_READS2 = json.load(f)

    samples = list(PREPROCESSED_TO_READS1.keys())

    with open(f"{OUTPUT_DIR}/data/bins_to_files.json", "r") as f:
        BINS_TO_FILES = json.load(f)

    rule all:
        input:
            *([f"{OUTPUT_DIR}/profiling_genomes/final/counts.tsv"] if "genomes" in PROFILING_TYPE else []),
            *([f"{OUTPUT_DIR}/profiling_genomes/final/bases.tsv"] if "genomes" in PROFILING_TYPE else []),
            *([f"{OUTPUT_DIR}/profiling_pangenomes/final/final.tsv"] if "pangenomes" in PROFILING_TYPE else [])

    if "genomes" in PROFILING_TYPE:
        include: "rules/profiling_genomes.smk"

    if "pangenomes" in PROFILING_TYPE:
        include: "rules/profiling_pangenomes.smk"

if WORKFLOW == "annotating":

    with open(f"{OUTPUT_DIR}/data/bins_to_files.json", "r") as f:
        BINS_TO_FILES = json.load(f)

    rule all:
        input:
            f"{OUTPUT_DIR}/annotating/gtdbtk/classify/gtdbtk.bac120.summary.tsv"
