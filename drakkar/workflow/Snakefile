import os
import gzip
import json
import pandas as pd
from glob import glob

####
# Define config variables
####

configfile: "workflow/config.yaml"
WORKFLOW = config.get("workflow", None)
OUTPUT_DIR = config.get("output_dir", None)
REFERENCE = config.get("reference", None)
FRACTION = config.get("fraction", None)
PROFILING_TYPE = config["profiling_type"].split(",") if "profiling_type" in config else []
ANNOTATING_TYPE = config["annotating_type"].split(",") if "annotating_type" in config else []

####
# Calculate optimal resources for computing
####

import os

def calculate_file_sizes(file_dict):
    """Calculates file sizes in megabytes (MB) for both single-file and list-based dictionaries."""
    file_sizes = {}

    for key, value in file_dict.items():
        total_size_mb = 0  # Initialize total size for this key

        # If value is a single string (single file path)
        if isinstance(value, str):
            files = [value]  # Convert to list to handle uniformly
        else:
            files = value  # Already a list

        # Process each file
        for file_path in files:
            if os.path.exists(file_path):  # Check if file exists
                file_size_mb = os.path.getsize(file_path) / 1e6  # Convert bytes to MB
                total_size_mb += file_size_mb
            else:
                print(f"⚠️ Warning: File not found - {file_path}")

        file_sizes[key] = total_size_mb  # Store total size for this key

    return file_sizes

####
# Create environments
####

if WORKFLOW == "environments":
    include: "rules/environments.smk"

####
# Run pipelines
####

if WORKFLOW == "preprocessing":

    with open(f"{OUTPUT_DIR}/data/sample_to_reads1.json", "r") as f:
        SAMPLE_TO_READS1 = json.load(f)

    with open(f"{OUTPUT_DIR}/data/sample_to_reads2.json", "r") as f:
        SAMPLE_TO_READS2 = json.load(f)

    samples = list(SAMPLE_TO_READS1.keys())

    # The rules reference_index, reference_map, metagenomic_reads and host_reads are only run if
    # the reference genome file is provided. Otherwise, the fastp rule already outputs the final files.

    if REFERENCE:

        with open(f"{OUTPUT_DIR}/data/reference_to_file.json", "r") as f:
            REFERENCE_TO_FILE = json.load(f)

        with open(f"{OUTPUT_DIR}/data/sample_to_reference.json", "r") as f:
            SAMPLE_TO_REFERENCE = json.load(f)

        references = list(REFERENCE_TO_FILE.keys())

        rule all:
            input:
                expand(f"{OUTPUT_DIR}/data/references/{{reference}}.fna", reference=references),
                expand(f"{OUTPUT_DIR}/data/references/{{reference}}.rev.1.bt2", reference=references),
                expand(f"{OUTPUT_DIR}/preprocessing/final/{{sample}}_1.fq.gz", sample=samples),
                expand(f"{OUTPUT_DIR}/preprocessing/final/{{sample}}_2.fq.gz", sample=samples),
                expand(f"{OUTPUT_DIR}/preprocessing/final/{{sample}}.bam", sample=samples),
                f"{OUTPUT_DIR}/preprocessing.tsv",
                f"{OUTPUT_DIR}/preprocessing/preprocessing.html",
                f"{OUTPUT_DIR}/drakkar_report.html"
                #f"{OUTPUT_DIR}/data/preprocessing_report.done"

        include: "rules/preparing.smk"
        include: "rules/preprocessing_ref.smk"

    if not REFERENCE:
        rule all:
            input:
                expand(f"{OUTPUT_DIR}/preprocessing/final/{{sample}}_1.fq.gz", sample=samples),
                expand(f"{OUTPUT_DIR}/preprocessing/final/{{sample}}_2.fq.gz", sample=samples),
                f"{OUTPUT_DIR}/preprocessing.tsv",
                f"{OUTPUT_DIR}/drakkar_report.html"
                #f"{OUTPUT_DIR}/data/preprocessing_report.done"

        include: "rules/preparing.smk"
        include: "rules/preprocessing.smk"

if WORKFLOW == "cataloging":

    with open(f"{OUTPUT_DIR}/data/preprocessed_to_reads1.json", "r") as f:
        PREPROCESSED_TO_READS1 = json.load(f)

    with open(f"{OUTPUT_DIR}/data/preprocessed_to_reads2.json", "r") as f:
        PREPROCESSED_TO_READS2 = json.load(f)

    with open(f"{OUTPUT_DIR}/data/assembly_to_samples.json", "r") as f:
        ASSEMBLY_TO_SAMPLES = json.load(f)

    preprocessed_mb = calculate_file_sizes(PREPROCESSED_TO_READS1)
    preprocessed_mb_total = sum(preprocessed_mb.values())

    samples = list(PREPROCESSED_TO_READS1.keys())
    assemblies = list(ASSEMBLY_TO_SAMPLES.keys())

    #Function to obtain bin_ids dynamically when they are generated by binette
    def get_bin_ids_from_tsv(tsv_path):     #it requires a checkpoint output in order to trigger the re-dagging
        df = pd.read_csv(tsv_path, sep="\t")
        return df["bin_id"].unique()

    def collect_bin_fastas(wildcards):
        fasta_list = []
        for asm in assemblies:
            # this will _run_ the checkpoint if it hasn't, and then return its outputs
            tsv = checkpoints.binette.get(assembly=asm).output[0]
            # now that the TSV is guaranteed to exist, read it
            for bin_id in get_bin_ids_from_tsv(tsv):
                fasta_list.append(
                    os.path.join(
                        OUTPUT_DIR, "cataloging", "final",
                        asm, f"{asm}_bin_{bin_id}.fa"
                    )
                )
        return fasta_list

    rule all:
        input:
            f"{OUTPUT_DIR}/cataloging/final/all_bin_paths.txt",
            f"{OUTPUT_DIR}/cataloging/final/all_bin_metadata.csv",
            #expand over multiple samples per assembly
            [f"{OUTPUT_DIR}/cataloging/bowtie2/{assembly}/{sample}.bam"
            for assembly, samples in ASSEMBLY_TO_SAMPLES.items()
            for sample in samples],
            collect_bin_fastas

    include: "rules/cataloging.smk"

if WORKFLOW == "profiling":

    with open(f"{OUTPUT_DIR}/data/preprocessed_to_reads1.json", "r") as f:
        PREPROCESSED_TO_READS1 = json.load(f)

    with open(f"{OUTPUT_DIR}/data/preprocessed_to_reads2.json", "r") as f:
        PREPROCESSED_TO_READS2 = json.load(f)

    samples = list(PREPROCESSED_TO_READS1.keys())

    with open(f"{OUTPUT_DIR}/data/bins_to_files.json", "r") as f:
        BINS_TO_FILES = json.load(f)

    bins = list(BINS_TO_FILES.keys())

    rule all:
        input:
            *([f"{OUTPUT_DIR}/profiling_genomes/final/counts.tsv"] if "genomes" in PROFILING_TYPE else []),
            *([f"{OUTPUT_DIR}/profiling_genomes/final/bases.tsv"] if "genomes" in PROFILING_TYPE else []),
            *([f"{OUTPUT_DIR}/profiling_genomes.tsv"] if "genomes" in PROFILING_TYPE else []),
            *([f"{OUTPUT_DIR}/profiling_genomes/singlem/microbial_fraction.tsv"] if ("genomes" in PROFILING_TYPE and FRACTION) else []),
            *([f"{OUTPUT_DIR}/profiling_genomes/genome_gene.csv"] if "pangenomes" in PROFILING_TYPE else []),
            *([f"{OUTPUT_DIR}/profiling_pangenomes/final/final.tsv"] if "pangenomes" in PROFILING_TYPE else [])

    if "genomes" in PROFILING_TYPE:
        include: "rules/profiling_genomes.smk"

    if "pangenomes" in PROFILING_TYPE:
        include: "rules/profiling_pangenomes.smk"

if WORKFLOW == "annotating":

    with open(f"{OUTPUT_DIR}/data/mags_to_files.json", "r") as f:
        MAGS_TO_FILES = json.load(f)

    mags = list(MAGS_TO_FILES.keys())

    rule all:
        input:
            *([f"{OUTPUT_DIR}/annotating/final/{mag}.tsv" for mag in mags] if "function" in ANNOTATING_TYPE else []),
            *([f"{OUTPUT_DIR}/annotating/genomad/{mag}/{mag}_summary/{mag}_virus_summary.tsv" for mag in mags] if "function" in ANNOTATING_TYPE else []),
            *([f"{OUTPUT_DIR}/annotating/dbcan/{mag}/substrate_prediction.tsv" for mag in mags] if "function" in ANNOTATING_TYPE else []),
            *([f"{OUTPUT_DIR}/annotating/gene_annotations.tsv.xz"] if "function" in ANNOTATING_TYPE else []),
            *([f"{OUTPUT_DIR}/annotating/genome_taxonomy.tsv"] if "taxonomy" in ANNOTATING_TYPE else []),
            #*([f"{OUTPUT_DIR}/annotating/m2m/{mag}.sbml" for mag in mags] if "network" in ANNOTATING_TYPE else []),
            *([f"{OUTPUT_DIR}/annotating/mergem/{mag}.sbml" for mag in mags] if "network" in ANNOTATING_TYPE else [])

    if "taxonomy" in ANNOTATING_TYPE:
        include: "rules/annotating_taxonomy.smk"

    if "function" in ANNOTATING_TYPE:
        include: "rules/annotating_function.smk"

    if "network" in ANNOTATING_TYPE:
        include: "rules/annotating_network.smk"

if WORKFLOW == "inspecting":

    with open(f"{OUTPUT_DIR}/data/microdiversity_selection.json", "r") as f:
        MICRODIVERSITY_SELECTION = json.load(f)

    genomes = list(MICRODIVERSITY_SELECTION.keys())

    rule all:
        input:
            *([f"{OUTPUT_DIR}/inspecting_microdiversity/variants/{genome}.vcf.gz" for genome in genomes] if "microdiversity" in PROFILING_TYPE else [])

    if "microdiversity" in INSPECTING_TYPE:
        include: "rules/inspecting_microdiversity.smk"


if WORKFLOW == "expressing":

    with open(f"{OUTPUT_DIR}/data/transcriptome_to_reads1.json", "r") as f:
        TRANSCRIPTOME_TO_READS1 = json.load(f)

    with open(f"{OUTPUT_DIR}/data/transcriptome_to_reads2.json", "r") as f:
        TRANSCRIPTOME_TO_READS2 = json.load(f)

    samples = list(TRANSCRIPTOME_TO_READS1.keys())

    with open(f"{OUTPUT_DIR}/data/mags_to_files.json", "r") as f:
        MAGS_TO_FILES = json.load(f)

    mags = list(MAGS_TO_FILES.keys())

    rule all:
        input:
            f"{OUTPUT_DIR}/expressing/featurecounts/counts.tsv"

    include: "rules/expressing.smk"